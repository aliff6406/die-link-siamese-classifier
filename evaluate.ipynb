{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork(backbone=\"efficientnet\")\n",
    "\n",
    "checkpoint_path = 'runs/efficientnet/bce/efficientnet/best.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bce(checkpoint_path, test_pairs, tensors):\n",
    "    out_path = 'eval_results/sam/batchsize128'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    # model = \"vgg128\"\n",
    "    csv = f\"{out_path}/batchsize128.csv\"\n",
    "    init_evaluate_log(csv)\n",
    "\n",
    "    model = SiameseNetwork()\n",
    "    model.to('cuda')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"epoch: \", checkpoint['epoch'])\n",
    "\n",
    "    # test_dataset = EvaluateImagePairDataset(test_pairs, tensors, transform='resnet50')\n",
    "    test_dataset = EvaluatePairDataset(test_pairs, tensors)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tensor1, tensor2, label, coin1, coin2 in test_loader:\n",
    "            tensor1, tensor2, label = map(lambda x: x.to('cuda'), [tensor1, tensor2, label])\n",
    "            label = label.view(-1)\n",
    "\n",
    "            prob = model(tensor1, tensor2).squeeze(1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend((prob > 0.5).cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    false_pr = float(fp/(fp+tn))\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "\n",
    "    write_csv(csv, [accuracy, precision, recall, f1, auc, false_pr, tpr, fpr])\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}')\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    print(f'Accuracy (from cm): {(tp+tn)/(tp+tn+fp+fn):.3f}')\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'FPR: {false_pr:.4f}')\n",
    "    print(f'F1 Score: {f1:.3f}')\n",
    "    print(f'AUC: {auc:.3f}')\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bce(\n",
    "    'runs/sam/bce/1c-128/best.pt',\n",
    "    'data/ccc_tensors/pair/combined/test_dataset.csv',\n",
    "    'data/ccc_tensors/data/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tripletsiamese import SiameseNetwork\n",
    "from datasets import EvaluateTripletDataset, OfflineTripletDataset\n",
    "from eval_metrics import evaluate, final_evaluate\n",
    "from utils import init_evaluate_log, write_csv\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triplet_threshold(checkpoint_path, test_pairs, tensors):\n",
    "    model = SiameseNetwork()\n",
    "    model.to('cuda')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"epoch: \", checkpoint['epoch'])\n",
    "\n",
    "    test_dataset = OfflineTripletDataset(test_pairs, tensors)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and labels\n",
    "    distances, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in test_loader:\n",
    "            anchor, positive, negative = map(lambda x: x.to('cuda'), [anchor, positive, negative])\n",
    "\n",
    "            anc_emb, pos_emb, neg_emb = model(anchor, positive, negative)\n",
    "                \n",
    "            ap_dist = F.pairwise_distance(anc_emb, pos_emb, p=2)\n",
    "            an_dist = F.pairwise_distance(anc_emb, neg_emb, p=2)\n",
    "\n",
    "            distances.extend(ap_dist.cpu().detach().numpy().flatten())\n",
    "            labels.extend(np.ones(ap_dist.size(0)).flatten())\n",
    "\n",
    "            distances.extend(an_dist.cpu().detach().numpy().flatten())\n",
    "            labels.extend(np.zeros(an_dist.size(0)).flatten())\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        distances = np.array(distances)\n",
    "\n",
    "        tpr, fpr, acc, threshold = evaluate(distances, labels)\n",
    "\n",
    "        print(f\"Optimal Threshold: {threshold}\")\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold = find_triplet_threshold(\n",
    "    'runs/sam/triplet/1i/best.pt',\n",
    "    'data/ccc_tensors/triplet/combined/val_dataset.csv',\n",
    "    'data/ccc_tensors/data'\n",
    ")\n",
    "# epoch:  38\n",
    "# Optimal Threshold: 36.43449380494487\n",
    "# Accuracy: 0.8578066914498141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_triplet(checkpoint_path, test_pairs, tensors):\n",
    "    out_path = 'eval_results/sam/triplet/1i'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    csv = f\"{out_path}/combined.csv\"\n",
    "    init_evaluate_log(csv)\n",
    "\n",
    "    model = SiameseNetwork()\n",
    "    model.to('cuda')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"epoch: \", checkpoint['epoch'])\n",
    "\n",
    "    test_dataset = EvaluateTripletDataset(test_pairs, tensors)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and labels\n",
    "    distances, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative, coin_anchor, coin_positive, coin_negative in test_loader:\n",
    "            anchor, positive, negative = map(lambda x: x.to('cuda'), [anchor, positive, negative])\n",
    "\n",
    "            anc_emb, pos_emb, neg_emb = model(anchor, positive, negative)\n",
    "                \n",
    "            ap_dist = F.pairwise_distance(anc_emb, pos_emb, p=2)\n",
    "            an_dist = F.pairwise_distance(anc_emb, neg_emb, p=2)\n",
    "\n",
    "            distances.extend(ap_dist.cpu().detach().numpy().flatten())\n",
    "            labels.extend(np.ones(ap_dist.size(0)).flatten())\n",
    "\n",
    "            distances.extend(an_dist.cpu().detach().numpy().flatten())\n",
    "            labels.extend(np.zeros(an_dist.size(0)).flatten())\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    distances = np.array(distances)\n",
    "\n",
    "    accuracy, precision, recall, fpr, f1 = final_evaluate(distances, labels, optimal_threshold)\n",
    "\n",
    "    auc = \"na\"\n",
    "\n",
    "    write_csv(csv, [accuracy, precision, recall, f1, auc, fpr, recall, fpr])\n",
    "\n",
    "    # Print metrics\n",
    "    # print(f'TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}')\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    # print(f'Accuracy (from cm): {(tp+tn)/(tp+tn+fp+fn):.3f}')\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'FPR: {fpr:.4f}')\n",
    "    print(f'F1 Score: {f1:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_triplet(\n",
    "    'runs/sam/triplet/1i/best.pt',\n",
    "    'data/ccc_tensors/triplet/combined/test_dataset.csv',\n",
    "    'data/ccc_tensors/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from samsiamese import SiameseNetwork\n",
    "from datasets import EvaluateTripletDataset, OfflineTripletDataset, OfflinePairDataset\n",
    "from eval_metrics import evaluate, final_evaluate\n",
    "from utils import init_evaluate_log, write_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contrastive_threshold(checkpoint_path, test_pairs, tensors):\n",
    "    model = SiameseNetwork(contrastive_loss=True)\n",
    "    model.to('cuda')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"epoch: \", checkpoint['epoch'])\n",
    "\n",
    "    test_dataset = OfflinePairDataset(test_pairs, tensors)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and labels\n",
    "    distances, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tensor1, tensor2, label in test_loader:\n",
    "            tensor1, tensor2, label = map(lambda x: x.to('cuda'), [tensor1, tensor2, label])\n",
    "            label = label.view(-1)\n",
    "\n",
    "            emb1, emb2 = model(tensor1, tensor2)\n",
    "            \n",
    "            distance = F.pairwise_distance(emb1, emb2, p=2)\n",
    "\n",
    "            distances.extend(distance.cpu().detach().numpy().flatten())\n",
    "            labels.extend(label.cpu().detach().numpy().flatten())\n",
    "\n",
    "\n",
    "        labels = np.array(labels).flatten()\n",
    "        distances = np.array(distances).flatten()\n",
    "\n",
    "        tpr, fpr, acc, threshold = evaluate(distances, labels)\n",
    "\n",
    "        print(f\"Optimal Threshold: {threshold}\")\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_threshold = find_contrastive_threshold(\n",
    "    'runs/sam/contrastive/1f/best.pt',\n",
    "    'data/ccc_tensors/pair/combined/val_dataset.csv',\n",
    "    'data/ccc_tensors/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_contrastive(checkpoint_path, test_pairs, tensors):\n",
    "    out_path = 'eval_results/sam/contrastive/1f'\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    csv = f\"{out_path}/combined.csv\"\n",
    "    init_evaluate_log(csv)\n",
    "\n",
    "    model = SiameseNetwork(contrastive_loss=True)\n",
    "    model.to('cuda')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"epoch: \", checkpoint['epoch'])\n",
    "\n",
    "    test_dataset = OfflinePairDataset(test_pairs, tensors)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and labels\n",
    "    distances, labels = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tensor1, tensor2, label in test_loader:\n",
    "            tensor1, tensor2, label = map(lambda x: x.to('cuda'), [tensor1, tensor2, label])\n",
    "            label = label.view(-1)\n",
    "\n",
    "            emb1, emb2 = model(tensor1, tensor2)\n",
    "            \n",
    "            distance = F.pairwise_distance(emb1, emb2, p=2)\n",
    "\n",
    "            distances.extend(distance.cpu().detach().numpy().flatten())\n",
    "            labels.extend(label.cpu().detach().numpy().flatten())\n",
    "\n",
    "\n",
    "        labels = np.array(labels).flatten()\n",
    "        distances = np.array(distances).flatten()\n",
    "\n",
    "    accuracy, precision, recall, fpr, f1 = final_evaluate(distances, labels, optimal_threshold)\n",
    "\n",
    "    # accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
    "    # precision = float(tp / (tp + fp))\n",
    "    # recall = float(tp / (tp + fn))\n",
    "    # f1 = float((2 * precision * recall) / (precision + recall))\n",
    "    # fpr = float(fp/(fp+tn))\n",
    "    # auc = roc_auc_score(all_labels, all_preds)\n",
    "    auc = \"na\"\n",
    "\n",
    "    write_csv(csv, [accuracy, precision, recall, f1, auc, fpr, recall, fpr])\n",
    "\n",
    "    # Print metrics\n",
    "    # print(f'TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}')\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    # print(f'Accuracy (from cm): {(tp+tn)/(tp+tn+fp+fn):.3f}')\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'FPR: {fpr:.4f}')\n",
    "    print(f'F1 Score: {f1:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_contrastive(\n",
    "    'runs/sam/contrastive/1f/best.pt',\n",
    "    'data/ccc_tensors/pair/combined/test_dataset.csv',\n",
    "    'data/ccc_tensors/data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
